{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fa73c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from frechetdist import frdist\n",
    "from IPython.utils import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "from rtree import index\n",
    "import scipy.spatial as spatial\n",
    "from scipy.interpolate import interp1d\n",
    "from shapely.geometry import LineString, Point\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree._tree import TREE_LEAF\n",
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO  \n",
    "import pydotplus\n",
    "from IPython.display import Image  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c19a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import sumolib\n",
    "import sys\n",
    "sys.path.append(r'/mnt/c/users/ty90rize/Sumo/tools') # path to sumolib\n",
    "sys.path.append(r'/mnt/c/users/ty90rize/repos/roadmatching/labelling_tool') # path to labelling tool\n",
    "import sumolib\n",
    "import preprocessing\n",
    "import strokeutils\n",
    "import similarity\n",
    "import inference_utils as iu\n",
    "\n",
    "\n",
    "path_train_osm_network = 'oktober_data/small_map/osm_sumo_data/2023-10-09-11-40-39/osm.net.xml/osm.net.xml'\n",
    "path_train_tomtom_network = 'oktober_data/small_map/tomtom_sumo_data/tomtom_sumo_small.net.xml'\n",
    "\n",
    "out_dir = 'out_dir' # where to put training file with similarity calculations\n",
    "preprocessed_file = 'preprocessed_df_2023-12-13.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_osm_train = sumolib.net.readNet(path_train_osm_network)\n",
    "net_tomtom_train = sumolib.net.readNet(path_train_tomtom_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73890a60",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059784ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "preprocess = True\n",
    "if Path(os.path.join(os.path.dirname(os.path.realpath('__file__')), out_dir, preprocessed_file)).is_file():\n",
    "    preprocess = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ff5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_candidate_df(net_osm, net_tomtom, radius, tomtom_rtree_idx, edgelist = []):\n",
    "\n",
    "    candidate_dfs = []\n",
    "    candidate_features = []\n",
    "    \n",
    "    if edgelist == []:\n",
    "        edgelist = net_osm.getEdges(withInternal=False)\n",
    "    else:\n",
    "        edgelist = [net_osm.getEdge(e) for e in copy.deepcopy(edgelist)]\n",
    "\n",
    "    \n",
    "    counter = 0\n",
    "    time_prev = time.time()\n",
    "\n",
    "    for edge in edgelist:\n",
    "        if counter < 100:\n",
    "            if counter < 10:\n",
    "                print('counter: {}'.format(counter))\n",
    "            if counter % 10 == 0:\n",
    "                now = time.time()\n",
    "                time_passed = now - time_prev\n",
    "                print('passed last 10 iteration in: {}'.format(round(time_passed, 2)))\n",
    "                time_prev = now\n",
    "\n",
    "        if counter % 100 == 0:\n",
    "            now = time.time()\n",
    "            time_passed = now - time_prev\n",
    "            print('passed last 100 iteration in: {}'.format(round(time_passed, 2)))\n",
    "            time_prev = now\n",
    "        reference_coordinates = preprocessing.get_transformed_coordinates(edge, net_osm, net_tomtom)\n",
    "        reference_id = edge.getID()\n",
    "        candidates = iu.get_candidates(reference_id, reference_coordinates, radius, tomtom_rtree_idx, net_tomtom, net_osm, False)\n",
    "        tomtom_ids = list(candidates.keys())  \n",
    "        if len(tomtom_ids) > 0:\n",
    "            osm_names = [reference_id for i in range(len(tomtom_ids))]\n",
    "            candidate_df = pd.DataFrame({'OSM':osm_names, 'Tomtom':tomtom_ids})\n",
    "\n",
    "            feature_names = candidates[tomtom_ids[0]].keys()\n",
    "            for name in feature_names:\n",
    "                candidate_df[name] = [candidates[tomtom_ids[i]][name] for i in range(len(tomtom_ids))]            \n",
    "\n",
    "            candidate_dfs.append(candidate_df)\n",
    "        counter += 1\n",
    "                \n",
    "    candidate_df = pd.concat(candidate_dfs).reset_index(drop = True)\n",
    "    \n",
    "    return candidate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded80a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(df, filename, target_dir = out_dir):\n",
    "    if not os.path.isdir(target_dir):\n",
    "        os.mkdir(target_dir)\n",
    "    df.to_json(os.path.join(target_dir, filename))\n",
    "\n",
    "def preprocess_data(net_osm, net_tomtom, radius, tomtom_rtree_idx, preprocessed_file, osm_edges = []):\n",
    "    candidate_df = create_candidate_df(net_osm, net_tomtom, radius, tomtom_rtree_idx, osm_edges)\n",
    "    print(preprocessed_file)\n",
    "    save_to_json(candidate_df, preprocessed_file)\n",
    "    return candidate_df\n",
    "\n",
    "def get_features(preprocess, net_osm, net_tomtom, radius, out_dir, preprocessed_file, osm_edges = []):\n",
    "    if preprocess:\n",
    "        tomtom_rtree_idx = iu.create_tomtom_index(net_tomtom)\n",
    "        print('completed tomtom rtree idx')\n",
    "        with io.capture_output() as captured:\n",
    "            candidate_df = preprocess_data(net_osm, net_tomtom, radius, tomtom_rtree_idx, preprocessed_file, osm_edges)\n",
    "    else:\n",
    "        candidate_df = pd.read_json(os.path.join(out_dir, preprocessed_file))\n",
    "        candidate_df['Tomtom'] = candidate_df['Tomtom'].astype('str')\n",
    "\n",
    "    return candidate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544e3a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c22aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "    \n",
    "candidate_df = get_features(preprocess, net_osm_train, net_tomtom_train, radius, out_dir, preprocessed_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c266a34b",
   "metadata": {},
   "source": [
    "## Build ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0fb9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_filedir = 'labelling_tool/data_fin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ground_truth_df(relative_filedir):\n",
    "    files = os.listdir(relative_filedir)\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        filepath = os.path.join(relative_filedir, file)\n",
    "        df = pd.read_csv(filepath)\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs).reset_index(drop = True)\n",
    "    df['matches'] = 1\n",
    "    df['OSM'] = df['OSM'].astype('str')\n",
    "    df['Tomtom'] = df['Tomtom'].astype('str')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32b521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = build_ground_truth_df(relative_filedir)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_edges_labelled = df['OSM'].unique()\n",
    "candidates_filtered = candidate_df[candidate_df['OSM'].isin(osm_edges_labelled)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df = pd.merge(candidates_filtered, df, how = 'left', left_on = ['OSM', 'Tomtom'], right_on = ['OSM', 'Tomtom'])\n",
    "ml_df['matches'] = ml_df['matches'].fillna(0) # ensure that NaNs become 0s\n",
    "ml_df['matches'] = ml_df['matches'].astype(int)\n",
    "ml_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63176b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = ml_df.groupby(['matches']).agg('count')\n",
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b8887",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df_orig = copy.deepcopy(ml_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74df684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df = ml_df[ml_df['overlap_shorter'] >= 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6bd9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df.groupby(['matches']).agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0213ab",
   "metadata": {},
   "source": [
    "## ML Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27829461",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import imblearn and other necessary packages\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c56b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a dataset named 'test_df' with a target column 'target'\n",
    "# Make sure to replace 'test_df' and 'target' with your actual dataset and target column name\n",
    "\n",
    "feature_columns = ['sinuosity_sim', 'cosine_sim', 'hausdorff_mod'\n",
    "                  ]\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = ml_df[feature_columns]\n",
    "feature_names = X.columns\n",
    "y = ml_df['matches']\n",
    "\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "md = 3\n",
    "clf = DecisionTreeClassifier(max_depth = md, min_samples_split = 10, criterion='entropy',) #class_weight = 'balanced')\n",
    "#clf = GradientBoostingClassifier(n_estimators = 25, max_depth = 2, min_samples_split = 10, learning_rate = 0.25)\n",
    "\n",
    "# Define the metrics you want to evaluate\n",
    "metrics = ['accuracy', 'precision', 'recall', 'roc_auc', 'balanced_accuracy', 'neg_brier_score', 'f1_macro', 'f1_weighted']\n",
    "\n",
    "# Perform 5-fold cross-validation and evaluate using the specified metrics\n",
    "cv_results = cross_validate(clf, X_resampled, y_resampled, cv=5, scoring=metrics)\n",
    "\n",
    "# Print the results\n",
    "for metric in metrics:\n",
    "    #print(f\"{metric.capitalize()} Scores for Each Fold:\")\n",
    "    #print(cv_results[f'test_{metric}'])\n",
    "    \n",
    "    # Calculate and print the mean score\n",
    "    mean_score = cv_results[f'test_{metric}'].mean()\n",
    "    print(f\"Mean {metric.capitalize()}: {mean_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ecb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth = md, min_samples_split = 10, criterion='entropy', #class_weight = 'balanced'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2557ea10",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84989414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "time = now.strftime(\"%y%m%d_%H%M%S\")\n",
    "\n",
    "from joblib import dump\n",
    "dump(clf, os.path.join(out_dir, 'dt_model_wo_strokes_{}.joblib'.format(time)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
